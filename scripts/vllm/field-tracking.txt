============================================================
vLLM Field Tracking Report — POST /v1/responses
============================================================

Total unique fields across all specs: 44

  Forwarded to vLLM:       16
  Not yet forwarded:        3
  Accepted, not forwarded:  6
  vLLM-only extensions:    17
  Not implemented:          2

——————————————————————————————————————————————————
FORWARDED (16): Fields accepted by gateway AND forwarded to vLLM
——————————————————————————————————————————————————
  frequency_penalty              [openai, gw-req, fwd]
  input                          [openai, vllm, gw-req, fwd]
  instructions                   [openai, vllm, gw-req, fwd]
  max_output_tokens              [openai, vllm, gw-req, fwd]
  model                          [openai, vllm, gw-req, fwd]
  parallel_tool_calls            [openai, vllm, gw-req, fwd]
  presence_penalty               [openai, gw-req, fwd]
  reasoning                      [openai, vllm, gw-req, fwd]
  store                          [openai, vllm, gw-req, fwd]
  stream                         [openai, vllm, gw-req, fwd]
  temperature                    [openai, vllm, gw-req, fwd]
  text                           [openai, vllm, gw-req, fwd]
  tool_choice                    [openai, vllm, gw-req, fwd]
  tools                          [openai, vllm, gw-req, fwd]
  top_p                          [openai, vllm, gw-req, fwd]
  truncation                     [openai, vllm, gw-req, fwd]

——————————————————————————————————————————————————
NOT_FORWARDED (3): Fields vLLM supports (and in OpenAI spec) but gateway does not forward yet
——————————————————————————————————————————————————
  background                     [openai, vllm]
  prompt_cache_key               [openai, vllm]
  service_tier                   [openai, vllm]

——————————————————————————————————————————————————
ACCEPTED_NOT_FORWARDED (6): Fields gateway accepts but does not forward to vLLM (handled by gateway or not yet wired)
——————————————————————————————————————————————————
  conversation                   [gw-req]
  include                        [openai, vllm, gw-req]
  max_tool_calls                 [openai, vllm, gw-req]
  metadata                       [openai, vllm, gw-req]
  previous_response_id           [openai, vllm, gw-req]
  top_logprobs                   [openai, vllm, gw-req]

——————————————————————————————————————————————————
VLLM_ONLY (17): vLLM-specific extensions not in the OpenAI spec
——————————————————————————————————————————————————
  cache_salt                     [vllm]
  enable_response_messages       [vllm]
  ignore_eos                     [vllm]
  include_stop_str_in_output     [vllm]
  logit_bias                     [vllm]
  mm_processor_kwargs            [vllm]
  previous_input_messages        [vllm]
  priority                       [vllm]
  prompt                         [vllm]
  repetition_penalty             [vllm]
  request_id                     [vllm]
  seed                           [vllm]
  skip_special_tokens            [vllm]
  stop                           [vllm]
  top_k                          [vllm]
  user                           [vllm]
  vllm_xargs                     [vllm]

——————————————————————————————————————————————————
NOT_IMPLEMENTED (2): Fields in OpenAI spec not yet accepted by gateway or forwarded to vLLM
——————————————————————————————————————————————————
  safety_identifier              [openai]
  stream_options                 [openai]

