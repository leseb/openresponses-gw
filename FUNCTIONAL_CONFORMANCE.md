# Functional Conformance Status

**Last Updated:** 2026-02-10

This document tracks the **actual implementation status** of the Responses API, distinguishing between:
- âœ… **Fully Implemented** - Parameter works as expected
- âš ï¸ **Schema Only** - Accepted and echoed, but NOT used in LLM calls
- ğŸ”„ **Mocked** - Simulated behavior for testing
- âŒ **Not Implemented** - Not supported at all

---

## API Conformance Summary

| Metric | Score | Notes |
|--------|-------|-------|
| **OpenAPI Schema Conformance** | 99.5% | OpenAPI spec matches OpenAI |
| **Functional Conformance** | ~35% | Many params accepted but ignored |
| **Endpoint Coverage** | 100% | All 41 endpoints schema-complete |

---

## Responses API - Parameter Implementation

### âœ… Fully Implemented (5 parameters)

| Parameter | Status | Implementation |
|-----------|--------|----------------|
| `model` | âœ… | Passed to LLM backend |
| `input` | âœ… | Converted to messages, passed to LLM |
| `instructions` | âœ… | Converted to system message |
| `temperature` | âœ… | Passed to LLM as-is |
| `max_output_tokens` | âœ… | Passed to LLM as `max_tokens` |

**Code Location:** `pkg/core/engine/engine.go:148-156`

```go
llmReq := &api.ChatCompletionRequest{
    Model:       model,           // âœ… Used
    Messages:    messages,        // âœ… Used
    Temperature: req.Temperature, // âœ… Used
    MaxTokens:   req.MaxOutputTokens, // âœ… Used
    Stream:      false,
}
```

---

### âš ï¸ Schema Only - NOT Passed to LLM (13 parameters)

These are **accepted, validated, and echoed** in the response, but **NOT used** in LLM calls:

| Parameter | Echoed? | Why Not Used |
|-----------|---------|--------------|
| `top_p` | âœ… Line 94 | Not in ChatCompletionRequest struct |
| `frequency_penalty` | âœ… Line 106 | Not in ChatCompletionRequest struct |
| `presence_penalty` | âœ… Line 109 | Not in ChatCompletionRequest struct |
| `truncation` | âœ… Line 112 | No direct chat completions equivalent |
| `top_logprobs` | âœ… Line 122 | Not in ChatCompletionRequest struct |
| `service_tier` | âœ… Line 125 | OpenAI-specific billing, not applicable |
| `background` | âœ… Line 128 | Not in ChatCompletionRequest struct |
| `parallel_tool_calls` | âœ… Line 100 | Not in ChatCompletionRequest struct |
| `store` | âœ… Line 103 | Session storage only, not LLM param |
| `prompt_cache_key` | âœ… Line 131 | Not in ChatCompletionRequest struct |
| `safety_identifier` | âœ… Line 132 | Not in ChatCompletionRequest struct |
| `metadata` | âœ… Line 133 | Stored locally, not sent to LLM |
| `include` | âœ… | Response filtering only, not LLM param |

**Impact:** Users can set these parameters, get them echoed back, but they have **no effect** on LLM behavior.

---

### ğŸ”„ Mocked/Simulated (2 features)

| Feature | Status | Implementation |
|---------|--------|----------------|
| `tools` | ğŸ”„ Mocked | **Fake tool calls generated** (line 174-189)<br/>Does NOT actually call LLM with tools |
| `tool_choice` | ğŸ”„ Echoed | Accepted but no real tool calling |

**Code Location:** `pkg/core/engine/engine.go:174-189`

```go
// If tools are provided, simulate a function call (for testing)
if len(req.Tools) > 0 {
    // Generate a function call output for the first tool
    tool := req.Tools[0]
    funcArgs := `{"location":"San Francisco, CA"}` // ğŸ”„ HARDCODED!
    resp.Output = []schema.ItemField{
        {
            Type:      "function_call",
            Name:      &tool.Name,
            Arguments: &funcArgs, // ğŸ”„ NOT FROM LLM!
        },
    }
}
```

**Impact:** Tool calling appears to work, but returns **fake data** without consulting the LLM.

---

### âŒ Not Implemented

| Feature | Status | Notes |
|---------|--------|-------|
| `previous_response_id` | âŒ | Stored but conversation history not loaded (line 137-144) |
| Multi-turn conversations | âŒ | Each request is independent |
| RAG / Vector Store integration | âŒ | Endpoints exist but return empty/stub data |
| File attachments in input | âŒ | Schema accepts but not processed |

---

## Endpoint Implementation Status

### Responses API (6/6 endpoints)

| Endpoint | Status | Notes |
|----------|--------|-------|
| POST /v1/responses | âœ… | Non-streaming & streaming (24 SSE events) |
| GET /v1/responses | âœ… | List with pagination (after, before, limit, order, model) |
| GET /v1/responses/{id} | âœ… | Retrieve stored response |
| DELETE /v1/responses/{id} | âœ… | Delete response |
| GET /v1/responses/{id}/input_items | âœ… | Retrieve input items |
| POST /responses | âœ… | Alias for /v1/responses (Open Responses spec) |

**Functional Status:**
- Request validation: âœ… OpenAPI schema enforced
- Response format: âœ… 100% spec compliant
- LLM integration: âœ… Translates to chat completions
- Parameter passthrough: âš ï¸ Only 5/18 params actually used in LLM calls

### Conversations API (6/6 endpoints)

| Endpoint | Status | Notes |
|----------|--------|-------|
| POST /v1/conversations | âœ… | Create conversation |
| GET /v1/conversations | âœ… | List with pagination |
| GET /v1/conversations/{id} | âœ… | Get conversation |
| DELETE /v1/conversations/{id} | âœ… | Delete conversation |
| POST /v1/conversations/{id}/items | âœ… | Add conversation items |
| GET /v1/conversations/{id}/items | âœ… | List conversation items |

### Models API (2/2 endpoints)

| Endpoint | Status | Notes |
|----------|--------|-------|
| GET /v1/models | âœ… | Returns available models |
| GET /v1/models/{id} | âœ… | Get specific model details |

### Prompts API (5/5 endpoints)

| Endpoint | Status | Notes |
|----------|--------|-------|
| POST /v1/prompts | âœ… | Create prompt template |
| GET /v1/prompts | âœ… | List prompts |
| GET /v1/prompts/{id} | âœ… | Get prompt |
| PUT /v1/prompts/{id} | âœ… | Update prompt |
| DELETE /v1/prompts/{id} | âœ… | Delete prompt |

### Files API (5/5 endpoints)

| Endpoint | Status | Notes |
|----------|--------|-------|
| POST /v1/files | âœ… | Upload works (multipart) |
| GET /v1/files | âœ… | List with pagination |
| GET /v1/files/{id} | âœ… | Metadata retrieval works |
| GET /v1/files/{id}/content | âœ… | Download works |
| DELETE /v1/files/{id} | âœ… | Deletion works |

**Limitation:** Files uploaded but not used in responses (no multimodal support yet).

### Vector Stores API (14/14 endpoints)

| Endpoint | Status | Notes |
|----------|--------|-------|
| POST /v1/vector_stores | âœ… | Create works |
| GET /v1/vector_stores | âœ… | List works |
| GET /v1/vector_stores/{id} | âœ… | Get works |
| PUT /v1/vector_stores/{id} | âœ… | Update works |
| DELETE /v1/vector_stores/{id} | âœ… | Delete works |
| POST /v1/vector_stores/{id}/files | âœ… | Add file works |
| GET /v1/vector_stores/{id}/files | âœ… | List files works |
| GET /v1/vector_stores/{id}/files/{file_id} | âœ… | Get file works |
| DELETE /v1/vector_stores/{id}/files/{file_id} | âœ… | Delete file works |
| GET /v1/vector_stores/{id}/files/{file_id}/content | âœ… | Get content works |
| POST /v1/vector_stores/{id}/search | ğŸ”„ | Endpoint works but returns stub data |
| POST /v1/vector_stores/{id}/file_batches | âœ… | Create batch works |
| GET /v1/vector_stores/{id}/file_batches/{batch_id} | âœ… | Get batch works |
| GET /v1/vector_stores/{id}/file_batches/{batch_id}/files | âœ… | List batch files works |
| POST /v1/vector_stores/{id}/file_batches/{batch_id}/cancel | âœ… | Cancel batch works |

**Limitations:**
- Search functionality: âŒ No actual vector embeddings or similarity search
- RAG integration: âŒ Not connected to responses API

---

## Testing Coverage

| Test Type | Status | Coverage |
|-----------|--------|----------|
| **OpenAPI Schema** | âœ… | 99.5% conformant |
| **Smoke Tests** | âœ… | 9 test suites pass |
| **Unit Tests** | âš ï¸ | Limited coverage |
| **Integration Tests** | âš ï¸ | Basic scenarios only |
| **Parameter Tests** | âŒ | No tests for ignored params |

---

## Recommendations

### High Priority Fixes

1. **Implement Core Parameters** (affects all users)
   - `top_p`, `frequency_penalty`, `presence_penalty`
   - Add to `ChatCompletionRequest` struct
   - Pass through to OpenAI SDK

2. **Fix Tool Calling** (currently broken)
   - Remove mock at line 174-189
   - Actually pass tools to LLM
   - Return real tool call results

3. **Document Limitations** (user expectations)
   - Add warnings to API docs
   - Return errors for unsupported features?
   - Or silently ignore (current behavior)

### Medium Priority

4. **Multi-turn Conversations**
   - Implement `previous_response_id` loading
   - Build conversation history from stored responses

5. **Add Parameter Tests**
   - Verify each param actually affects LLM output
   - Test that ignored params are documented

### Low Priority

6. **Advanced Features**
   - Response format (JSON mode)
   - Seed for reproducibility
   - Stop sequences
   - Log probabilities

---

## Known Gaps vs OpenAI

| Feature | OpenAI | This Gateway | Gap |
|---------|--------|--------------|-----|
| Parameter support | ~40 params | 5 functional | 87% ignored |
| Tool calling | âœ… Real | ğŸ”„ Mocked | Not functional |
| Multi-turn | âœ… Real | âŒ Stub | Not implemented |
| RAG/Search | âœ… Real | âŒ Stub | Not implemented |
| Vision | âœ… Real | âŒ None | Not implemented |
| Streaming | âœ… Real | âœ… Real | âœ… Works! |

---

## Architecture Clarity

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Responses API Request                              â”‚
â”‚ (18+ parameters accepted via OpenAPI schema)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine.ProcessRequest()                            â”‚
â”‚ â€¢ Echoes all params to response âœ…                 â”‚
â”‚ â€¢ Only uses 5 params for LLM âš ï¸                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatCompletionRequest                              â”‚
â”‚ â€¢ model, messages, temperature,                    â”‚
â”‚   max_tokens, stream                               â”‚
â”‚ â€¢ Missing: top_p, penalties, tools, etc.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI Client (openai-go SDK)                      â”‚
â”‚ â€¢ Could support 40+ params                         â”‚
â”‚ â€¢ We only pass 5                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
            [LLM Backend]
```

**The Gap:** We accept everything, echo everything, but only **use 5 parameters**.

---

## Version History

- **2026-02-10**: Updated endpoint coverage
  - Added 3 missing Responses API endpoints (list, delete, input_items)
  - All 41 endpoints now schema-complete (100%)
  - Functional implementation still ~35% (parameter limitations remain)

- **2026-02-09**: Initial functional conformance audit
  - OpenAPI schema: 99.5% âœ…
  - Functional implementation: ~35% âš ï¸
  - Gap identified and documented

---

## See Also

- [CONFORMANCE.md](./CONFORMANCE.md) - Open Responses spec conformance (100%)
- [CONFORMANCE_STATUS.md](./CONFORMANCE_STATUS.md) - OpenAPI conformance vs OpenAI
- [TESTING.md](./TESTING.md) - Testing guide
- [PROJECT_PLAN.md](./PROJECT_PLAN.md) - Implementation roadmap
